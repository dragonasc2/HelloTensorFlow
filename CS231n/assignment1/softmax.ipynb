{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.387725\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -0.490098 analytic: -0.490098, relative error: 8.954120e-08\n",
      "numerical: 0.920436 analytic: 0.920436, relative error: 3.992184e-08\n",
      "numerical: 0.106874 analytic: 0.106874, relative error: 4.312548e-07\n",
      "numerical: -0.326748 analytic: -0.326748, relative error: 5.227922e-08\n",
      "numerical: 1.080592 analytic: 1.080592, relative error: 1.853036e-09\n",
      "numerical: -0.114408 analytic: -0.114408, relative error: 4.867980e-07\n",
      "numerical: -0.513709 analytic: -0.513709, relative error: 7.361260e-08\n",
      "numerical: -2.206590 analytic: -2.206590, relative error: 1.281594e-08\n",
      "numerical: 2.524403 analytic: 2.524403, relative error: 1.353723e-08\n",
      "numerical: 3.690013 analytic: 3.690013, relative error: 2.347919e-08\n",
      "numerical: -0.568561 analytic: -0.568561, relative error: 5.541324e-09\n",
      "numerical: -1.055598 analytic: -1.055598, relative error: 3.308141e-09\n",
      "numerical: 0.680360 analytic: 0.680360, relative error: 2.743935e-08\n",
      "numerical: 0.037958 analytic: 0.037958, relative error: 5.197726e-07\n",
      "numerical: -0.935379 analytic: -0.935379, relative error: 7.657668e-09\n",
      "numerical: -1.751284 analytic: -1.751284, relative error: 9.934182e-09\n",
      "numerical: -0.057717 analytic: -0.057718, relative error: 3.604806e-07\n",
      "numerical: 1.716539 analytic: 1.716539, relative error: 2.330072e-08\n",
      "numerical: -0.277045 analytic: -0.277045, relative error: 1.691414e-07\n",
      "numerical: -0.926729 analytic: -0.926729, relative error: 9.499588e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.387725e+00 computed in 0.009007s\n",
      "vectorized loss: 2.387725e+00 computed in 0.006004s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 20000: loss 784.000056\n",
      "iteration 100 / 20000: loss 287.768525\n",
      "iteration 200 / 20000: loss 106.501621\n",
      "iteration 300 / 20000: loss 40.296039\n",
      "iteration 400 / 20000: loss 16.073383\n",
      "iteration 500 / 20000: loss 7.147751\n",
      "iteration 600 / 20000: loss 3.945465\n",
      "iteration 700 / 20000: loss 2.787431\n",
      "iteration 800 / 20000: loss 2.355985\n",
      "iteration 900 / 20000: loss 2.110091\n",
      "iteration 1000 / 20000: loss 2.137311\n",
      "iteration 1100 / 20000: loss 2.077630\n",
      "iteration 1200 / 20000: loss 2.146288\n",
      "iteration 1300 / 20000: loss 2.079717\n",
      "iteration 1400 / 20000: loss 2.109289\n",
      "iteration 1500 / 20000: loss 2.128071\n",
      "iteration 1600 / 20000: loss 2.058745\n",
      "iteration 1700 / 20000: loss 2.088253\n",
      "iteration 1800 / 20000: loss 2.080295\n",
      "iteration 1900 / 20000: loss 2.121006\n",
      "iteration 2000 / 20000: loss 2.060915\n",
      "iteration 2100 / 20000: loss 2.054522\n",
      "iteration 2200 / 20000: loss 2.092123\n",
      "iteration 2300 / 20000: loss 2.070684\n",
      "iteration 2400 / 20000: loss 2.068726\n",
      "iteration 2500 / 20000: loss 2.050817\n",
      "iteration 2600 / 20000: loss 2.101275\n",
      "iteration 2700 / 20000: loss 2.031801\n",
      "iteration 2800 / 20000: loss 2.101379\n",
      "iteration 2900 / 20000: loss 2.098961\n",
      "iteration 3000 / 20000: loss 2.095118\n",
      "iteration 3100 / 20000: loss 2.095895\n",
      "iteration 3200 / 20000: loss 2.080697\n",
      "iteration 3300 / 20000: loss 2.076078\n",
      "iteration 3400 / 20000: loss 2.155781\n",
      "iteration 3500 / 20000: loss 2.110231\n",
      "iteration 3600 / 20000: loss 2.068056\n",
      "iteration 3700 / 20000: loss 2.138554\n",
      "iteration 3800 / 20000: loss 2.081718\n",
      "iteration 3900 / 20000: loss 2.086508\n",
      "iteration 4000 / 20000: loss 2.123079\n",
      "iteration 4100 / 20000: loss 2.045725\n",
      "iteration 4200 / 20000: loss 2.062324\n",
      "iteration 4300 / 20000: loss 2.139439\n",
      "iteration 4400 / 20000: loss 2.066255\n",
      "iteration 4500 / 20000: loss 2.026535\n",
      "iteration 4600 / 20000: loss 2.077369\n",
      "iteration 4700 / 20000: loss 2.099103\n",
      "iteration 4800 / 20000: loss 2.016197\n",
      "iteration 4900 / 20000: loss 2.061919\n",
      "iteration 5000 / 20000: loss 2.123580\n",
      "iteration 5100 / 20000: loss 2.031906\n",
      "iteration 5200 / 20000: loss 2.048454\n",
      "iteration 5300 / 20000: loss 2.121755\n",
      "iteration 5400 / 20000: loss 2.125063\n",
      "iteration 5500 / 20000: loss 2.110136\n",
      "iteration 5600 / 20000: loss 2.133522\n",
      "iteration 5700 / 20000: loss 2.127441\n",
      "iteration 5800 / 20000: loss 2.093994\n",
      "iteration 5900 / 20000: loss 2.099485\n",
      "iteration 6000 / 20000: loss 2.042742\n",
      "iteration 6100 / 20000: loss 2.098792\n",
      "iteration 6200 / 20000: loss 2.068939\n",
      "iteration 6300 / 20000: loss 2.123513\n",
      "iteration 6400 / 20000: loss 2.078805\n",
      "iteration 6500 / 20000: loss 2.121027\n",
      "iteration 6600 / 20000: loss 2.055856\n",
      "iteration 6700 / 20000: loss 2.120696\n",
      "iteration 6800 / 20000: loss 2.163533\n",
      "iteration 6900 / 20000: loss 2.060033\n",
      "iteration 7000 / 20000: loss 2.055760\n",
      "iteration 7100 / 20000: loss 2.087585\n",
      "iteration 7200 / 20000: loss 2.061674\n",
      "iteration 7300 / 20000: loss 2.069660\n",
      "iteration 7400 / 20000: loss 2.093707\n",
      "iteration 7500 / 20000: loss 2.093535\n",
      "iteration 7600 / 20000: loss 2.138634\n",
      "iteration 7700 / 20000: loss 2.037703\n",
      "iteration 7800 / 20000: loss 2.132816\n",
      "iteration 7900 / 20000: loss 2.032944\n",
      "iteration 8000 / 20000: loss 2.117520\n",
      "iteration 8100 / 20000: loss 2.109041\n",
      "iteration 8200 / 20000: loss 2.076971\n",
      "iteration 8300 / 20000: loss 2.020695\n",
      "iteration 8400 / 20000: loss 2.122712\n",
      "iteration 8500 / 20000: loss 2.052433\n",
      "iteration 8600 / 20000: loss 2.112984\n",
      "iteration 8700 / 20000: loss 2.135243\n",
      "iteration 8800 / 20000: loss 2.075045\n",
      "iteration 8900 / 20000: loss 2.147715\n",
      "iteration 9000 / 20000: loss 2.041244\n",
      "iteration 9100 / 20000: loss 2.107549\n",
      "iteration 9200 / 20000: loss 2.096865\n",
      "iteration 9300 / 20000: loss 2.122210\n",
      "iteration 9400 / 20000: loss 2.129693\n",
      "iteration 9500 / 20000: loss 2.077804\n",
      "iteration 9600 / 20000: loss 2.081042\n",
      "iteration 9700 / 20000: loss 2.091120\n",
      "iteration 9800 / 20000: loss 2.113429\n",
      "iteration 9900 / 20000: loss 2.032021\n",
      "iteration 10000 / 20000: loss 2.052132\n",
      "iteration 10100 / 20000: loss 2.107983\n",
      "iteration 10200 / 20000: loss 2.001840\n",
      "iteration 10300 / 20000: loss 2.094105\n",
      "iteration 10400 / 20000: loss 2.099894\n",
      "iteration 10500 / 20000: loss 2.068248\n",
      "iteration 10600 / 20000: loss 2.130201\n",
      "iteration 10700 / 20000: loss 2.059397\n",
      "iteration 10800 / 20000: loss 2.075111\n",
      "iteration 10900 / 20000: loss 2.096694\n",
      "iteration 11000 / 20000: loss 2.043041\n",
      "iteration 11100 / 20000: loss 2.075095\n",
      "iteration 11200 / 20000: loss 2.114806\n",
      "iteration 11300 / 20000: loss 2.093455\n",
      "iteration 11400 / 20000: loss 2.111956\n",
      "iteration 11500 / 20000: loss 2.089163\n",
      "iteration 11600 / 20000: loss 2.160908\n",
      "iteration 11700 / 20000: loss 2.035510\n",
      "iteration 11800 / 20000: loss 2.065608\n",
      "iteration 11900 / 20000: loss 2.087220\n",
      "iteration 12000 / 20000: loss 2.130303\n",
      "iteration 12100 / 20000: loss 2.086841\n",
      "iteration 12200 / 20000: loss 2.094115\n",
      "iteration 12300 / 20000: loss 2.065568\n",
      "iteration 12400 / 20000: loss 2.090927\n",
      "iteration 12500 / 20000: loss 2.072159\n",
      "iteration 12600 / 20000: loss 2.143336\n",
      "iteration 12700 / 20000: loss 2.078923\n",
      "iteration 12800 / 20000: loss 2.029463\n",
      "iteration 12900 / 20000: loss 2.061340\n",
      "iteration 13000 / 20000: loss 2.101821\n",
      "iteration 13100 / 20000: loss 2.103560\n",
      "iteration 13200 / 20000: loss 2.064262\n",
      "iteration 13300 / 20000: loss 2.090610\n",
      "iteration 13400 / 20000: loss 2.070315\n",
      "iteration 13500 / 20000: loss 2.084762\n",
      "iteration 13600 / 20000: loss 2.024822\n",
      "iteration 13700 / 20000: loss 2.108412\n",
      "iteration 13800 / 20000: loss 2.049343\n",
      "iteration 13900 / 20000: loss 2.060318\n",
      "iteration 14000 / 20000: loss 2.095271\n",
      "iteration 14100 / 20000: loss 2.048604\n",
      "iteration 14200 / 20000: loss 2.057361\n",
      "iteration 14300 / 20000: loss 2.073078\n",
      "iteration 14400 / 20000: loss 2.138629\n",
      "iteration 14500 / 20000: loss 2.022223\n",
      "iteration 14600 / 20000: loss 2.053453\n",
      "iteration 14700 / 20000: loss 2.083707\n",
      "iteration 14800 / 20000: loss 2.085119\n",
      "iteration 14900 / 20000: loss 2.047909\n",
      "iteration 15000 / 20000: loss 2.130710\n",
      "iteration 15100 / 20000: loss 2.139844\n",
      "iteration 15200 / 20000: loss 2.050674\n",
      "iteration 15300 / 20000: loss 2.049157\n",
      "iteration 15400 / 20000: loss 2.110238\n",
      "iteration 15500 / 20000: loss 1.975880\n",
      "iteration 15600 / 20000: loss 2.101403\n",
      "iteration 15700 / 20000: loss 2.078659\n",
      "iteration 15800 / 20000: loss 2.105379\n",
      "iteration 15900 / 20000: loss 2.079147\n",
      "iteration 16000 / 20000: loss 2.050899\n",
      "iteration 16100 / 20000: loss 2.077011\n",
      "iteration 16200 / 20000: loss 2.098161\n",
      "iteration 16300 / 20000: loss 2.081116\n",
      "iteration 16400 / 20000: loss 2.113614\n",
      "iteration 16500 / 20000: loss 2.017617\n",
      "iteration 16600 / 20000: loss 2.082423\n",
      "iteration 16700 / 20000: loss 2.099549\n",
      "iteration 16800 / 20000: loss 2.112414\n",
      "iteration 16900 / 20000: loss 2.077194\n",
      "iteration 17000 / 20000: loss 2.106767\n",
      "iteration 17100 / 20000: loss 2.103013\n",
      "iteration 17200 / 20000: loss 2.101010\n",
      "iteration 17300 / 20000: loss 2.087935\n",
      "iteration 17400 / 20000: loss 2.054673\n",
      "iteration 17500 / 20000: loss 2.053766\n",
      "iteration 17600 / 20000: loss 2.078992\n",
      "iteration 17700 / 20000: loss 2.113151\n",
      "iteration 17800 / 20000: loss 2.082569\n",
      "iteration 17900 / 20000: loss 2.079605\n",
      "iteration 18000 / 20000: loss 2.154701\n",
      "iteration 18100 / 20000: loss 2.068991\n",
      "iteration 18200 / 20000: loss 2.095611\n",
      "iteration 18300 / 20000: loss 2.103515\n",
      "iteration 18400 / 20000: loss 2.046847\n",
      "iteration 18500 / 20000: loss 2.101880\n",
      "iteration 18600 / 20000: loss 2.045555\n",
      "iteration 18700 / 20000: loss 2.092947\n",
      "iteration 18800 / 20000: loss 2.077285\n",
      "iteration 18900 / 20000: loss 2.058772\n",
      "iteration 19000 / 20000: loss 2.085609\n",
      "iteration 19100 / 20000: loss 2.047021\n",
      "iteration 19200 / 20000: loss 2.116725\n",
      "iteration 19300 / 20000: loss 2.122414\n",
      "iteration 19400 / 20000: loss 2.023866\n",
      "iteration 19500 / 20000: loss 2.028054\n",
      "iteration 19600 / 20000: loss 2.108270\n",
      "iteration 19700 / 20000: loss 2.125163\n",
      "iteration 19800 / 20000: loss 2.086687\n",
      "iteration 19900 / 20000: loss 2.093338\n",
      "iteration 0 / 20000: loss 1547.401739\n",
      "iteration 100 / 20000: loss 208.502973\n",
      "iteration 200 / 20000: loss 29.689774\n",
      "iteration 300 / 20000: loss 5.796666\n",
      "iteration 400 / 20000: loss 2.606521\n",
      "iteration 500 / 20000: loss 2.244053\n",
      "iteration 600 / 20000: loss 2.145864\n",
      "iteration 700 / 20000: loss 2.110900\n",
      "iteration 800 / 20000: loss 2.171644\n",
      "iteration 900 / 20000: loss 2.140178\n",
      "iteration 1000 / 20000: loss 2.155229\n",
      "iteration 1100 / 20000: loss 2.135770\n",
      "iteration 1200 / 20000: loss 2.170252\n",
      "iteration 1300 / 20000: loss 2.146702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400 / 20000: loss 2.135047\n",
      "iteration 1500 / 20000: loss 2.147927\n",
      "iteration 1600 / 20000: loss 2.141838\n",
      "iteration 1700 / 20000: loss 2.181016\n",
      "iteration 1800 / 20000: loss 2.149450\n",
      "iteration 1900 / 20000: loss 2.131404\n",
      "iteration 2000 / 20000: loss 2.200385\n",
      "iteration 2100 / 20000: loss 2.142321\n",
      "iteration 2200 / 20000: loss 2.115741\n",
      "iteration 2300 / 20000: loss 2.168108\n",
      "iteration 2400 / 20000: loss 2.163405\n",
      "iteration 2500 / 20000: loss 2.182361\n",
      "iteration 2600 / 20000: loss 2.117820\n",
      "iteration 2700 / 20000: loss 2.174505\n",
      "iteration 2800 / 20000: loss 2.109293\n",
      "iteration 2900 / 20000: loss 2.109915\n",
      "iteration 3000 / 20000: loss 2.148201\n",
      "iteration 3100 / 20000: loss 2.150062\n",
      "iteration 3200 / 20000: loss 2.147734\n",
      "iteration 3300 / 20000: loss 2.106601\n",
      "iteration 3400 / 20000: loss 2.190383\n",
      "iteration 3500 / 20000: loss 2.142451\n",
      "iteration 3600 / 20000: loss 2.158582\n",
      "iteration 3700 / 20000: loss 2.149934\n",
      "iteration 3800 / 20000: loss 2.117997\n",
      "iteration 3900 / 20000: loss 2.154112\n",
      "iteration 4000 / 20000: loss 2.099048\n",
      "iteration 4100 / 20000: loss 2.144493\n",
      "iteration 4200 / 20000: loss 2.145762\n",
      "iteration 4300 / 20000: loss 2.125887\n",
      "iteration 4400 / 20000: loss 2.159921\n",
      "iteration 4500 / 20000: loss 2.148593\n",
      "iteration 4600 / 20000: loss 2.141273\n",
      "iteration 4700 / 20000: loss 2.138410\n",
      "iteration 4800 / 20000: loss 2.120070\n",
      "iteration 4900 / 20000: loss 2.153093\n",
      "iteration 5000 / 20000: loss 2.141031\n",
      "iteration 5100 / 20000: loss 2.122873\n",
      "iteration 5200 / 20000: loss 2.152141\n",
      "iteration 5300 / 20000: loss 2.144016\n",
      "iteration 5400 / 20000: loss 2.156290\n",
      "iteration 5500 / 20000: loss 2.120956\n",
      "iteration 5600 / 20000: loss 2.105311\n",
      "iteration 5700 / 20000: loss 2.141656\n",
      "iteration 5800 / 20000: loss 2.133870\n",
      "iteration 5900 / 20000: loss 2.140565\n",
      "iteration 6000 / 20000: loss 2.191796\n",
      "iteration 6100 / 20000: loss 2.146246\n",
      "iteration 6200 / 20000: loss 2.137326\n",
      "iteration 6300 / 20000: loss 2.194782\n",
      "iteration 6400 / 20000: loss 2.190104\n",
      "iteration 6500 / 20000: loss 2.148111\n",
      "iteration 6600 / 20000: loss 2.149824\n",
      "iteration 6700 / 20000: loss 2.185458\n",
      "iteration 6800 / 20000: loss 2.189726\n",
      "iteration 6900 / 20000: loss 2.172808\n",
      "iteration 7000 / 20000: loss 2.161678\n",
      "iteration 7100 / 20000: loss 2.161288\n",
      "iteration 7200 / 20000: loss 2.170197\n",
      "iteration 7300 / 20000: loss 2.096260\n",
      "iteration 7400 / 20000: loss 2.172265\n",
      "iteration 7500 / 20000: loss 2.101482\n",
      "iteration 7600 / 20000: loss 2.130707\n",
      "iteration 7700 / 20000: loss 2.093425\n",
      "iteration 7800 / 20000: loss 2.145657\n",
      "iteration 7900 / 20000: loss 2.122709\n",
      "iteration 8000 / 20000: loss 2.130837\n",
      "iteration 8100 / 20000: loss 2.157178\n",
      "iteration 8200 / 20000: loss 2.174552\n",
      "iteration 8300 / 20000: loss 2.146940\n",
      "iteration 8400 / 20000: loss 2.114831\n",
      "iteration 8500 / 20000: loss 2.170319\n",
      "iteration 8600 / 20000: loss 2.162788\n",
      "iteration 8700 / 20000: loss 2.158895\n",
      "iteration 8800 / 20000: loss 2.121507\n",
      "iteration 8900 / 20000: loss 2.115206\n",
      "iteration 9000 / 20000: loss 2.112729\n",
      "iteration 9100 / 20000: loss 2.053215\n",
      "iteration 9200 / 20000: loss 2.182336\n",
      "iteration 9300 / 20000: loss 2.159860\n",
      "iteration 9400 / 20000: loss 2.055685\n",
      "iteration 9500 / 20000: loss 2.172284\n",
      "iteration 9600 / 20000: loss 2.137532\n",
      "iteration 9700 / 20000: loss 2.133333\n",
      "iteration 9800 / 20000: loss 2.166876\n",
      "iteration 9900 / 20000: loss 2.155728\n",
      "iteration 10000 / 20000: loss 2.142955\n",
      "iteration 10100 / 20000: loss 2.110757\n",
      "iteration 10200 / 20000: loss 2.106276\n",
      "iteration 10300 / 20000: loss 2.090065\n",
      "iteration 10400 / 20000: loss 2.174997\n",
      "iteration 10500 / 20000: loss 2.132280\n",
      "iteration 10600 / 20000: loss 2.145572\n",
      "iteration 10700 / 20000: loss 2.115170\n",
      "iteration 10800 / 20000: loss 2.121224\n",
      "iteration 10900 / 20000: loss 2.125246\n",
      "iteration 11000 / 20000: loss 2.105675\n",
      "iteration 11100 / 20000: loss 2.133539\n",
      "iteration 11200 / 20000: loss 2.160052\n",
      "iteration 11300 / 20000: loss 2.142188\n",
      "iteration 11400 / 20000: loss 2.116897\n",
      "iteration 11500 / 20000: loss 2.139322\n",
      "iteration 11600 / 20000: loss 2.141462\n",
      "iteration 11700 / 20000: loss 2.125913\n",
      "iteration 11800 / 20000: loss 2.200274\n",
      "iteration 11900 / 20000: loss 2.114324\n",
      "iteration 12000 / 20000: loss 2.090993\n",
      "iteration 12100 / 20000: loss 2.124115\n",
      "iteration 12200 / 20000: loss 2.139187\n",
      "iteration 12300 / 20000: loss 2.158384\n",
      "iteration 12400 / 20000: loss 2.126673\n",
      "iteration 12500 / 20000: loss 2.198705\n",
      "iteration 12600 / 20000: loss 2.100249\n",
      "iteration 12700 / 20000: loss 2.145556\n",
      "iteration 12800 / 20000: loss 2.153467\n",
      "iteration 12900 / 20000: loss 2.228964\n",
      "iteration 13000 / 20000: loss 2.154421\n",
      "iteration 13100 / 20000: loss 2.151443\n",
      "iteration 13200 / 20000: loss 2.160805\n",
      "iteration 13300 / 20000: loss 2.175740\n",
      "iteration 13400 / 20000: loss 2.151304\n",
      "iteration 13500 / 20000: loss 2.098574\n",
      "iteration 13600 / 20000: loss 2.146727\n",
      "iteration 13700 / 20000: loss 2.121483\n",
      "iteration 13800 / 20000: loss 2.116812\n",
      "iteration 13900 / 20000: loss 2.162030\n",
      "iteration 14000 / 20000: loss 2.123646\n",
      "iteration 14100 / 20000: loss 2.174319\n",
      "iteration 14200 / 20000: loss 2.114512\n",
      "iteration 14300 / 20000: loss 2.151373\n",
      "iteration 14400 / 20000: loss 2.135508\n",
      "iteration 14500 / 20000: loss 2.171289\n",
      "iteration 14600 / 20000: loss 2.130230\n",
      "iteration 14700 / 20000: loss 2.140702\n",
      "iteration 14800 / 20000: loss 2.138042\n",
      "iteration 14900 / 20000: loss 2.168327\n",
      "iteration 15000 / 20000: loss 2.148938\n",
      "iteration 15100 / 20000: loss 2.141624\n",
      "iteration 15200 / 20000: loss 2.188731\n",
      "iteration 15300 / 20000: loss 2.144145\n",
      "iteration 15400 / 20000: loss 2.153629\n",
      "iteration 15500 / 20000: loss 2.152450\n",
      "iteration 15600 / 20000: loss 2.113737\n",
      "iteration 15700 / 20000: loss 2.129266\n",
      "iteration 15800 / 20000: loss 2.147433\n",
      "iteration 15900 / 20000: loss 2.084880\n",
      "iteration 16000 / 20000: loss 2.112036\n",
      "iteration 16100 / 20000: loss 2.171953\n",
      "iteration 16200 / 20000: loss 2.103368\n",
      "iteration 16300 / 20000: loss 2.178973\n",
      "iteration 16400 / 20000: loss 2.122122\n",
      "iteration 16500 / 20000: loss 2.124417\n",
      "iteration 16600 / 20000: loss 2.122181\n",
      "iteration 16700 / 20000: loss 2.127450\n",
      "iteration 16800 / 20000: loss 2.149035\n",
      "iteration 16900 / 20000: loss 2.146688\n",
      "iteration 17000 / 20000: loss 2.180688\n",
      "iteration 17100 / 20000: loss 2.139306\n",
      "iteration 17200 / 20000: loss 2.119541\n",
      "iteration 17300 / 20000: loss 2.204285\n",
      "iteration 17400 / 20000: loss 2.134739\n",
      "iteration 17500 / 20000: loss 2.145123\n",
      "iteration 17600 / 20000: loss 2.145179\n",
      "iteration 17700 / 20000: loss 2.091398\n",
      "iteration 17800 / 20000: loss 2.165192\n",
      "iteration 17900 / 20000: loss 2.187660\n",
      "iteration 18000 / 20000: loss 2.149275\n",
      "iteration 18100 / 20000: loss 2.155462\n",
      "iteration 18200 / 20000: loss 2.144627\n",
      "iteration 18300 / 20000: loss 2.193391\n",
      "iteration 18400 / 20000: loss 2.187484\n",
      "iteration 18500 / 20000: loss 2.130378\n",
      "iteration 18600 / 20000: loss 2.109946\n",
      "iteration 18700 / 20000: loss 2.126169\n",
      "iteration 18800 / 20000: loss 2.134758\n",
      "iteration 18900 / 20000: loss 2.133052\n",
      "iteration 19000 / 20000: loss 2.145642\n",
      "iteration 19100 / 20000: loss 2.127176\n",
      "iteration 19200 / 20000: loss 2.157095\n",
      "iteration 19300 / 20000: loss 2.152680\n",
      "iteration 19400 / 20000: loss 2.110905\n",
      "iteration 19500 / 20000: loss 2.105593\n",
      "iteration 19600 / 20000: loss 2.168308\n",
      "iteration 19700 / 20000: loss 2.122869\n",
      "iteration 19800 / 20000: loss 2.081223\n",
      "iteration 19900 / 20000: loss 2.115573\n",
      "iteration 0 / 20000: loss 775.077310\n",
      "iteration 100 / 20000: loss 6.891031\n",
      "iteration 200 / 20000: loss 2.124263\n",
      "iteration 300 / 20000: loss 2.051286\n",
      "iteration 400 / 20000: loss 2.020489\n",
      "iteration 500 / 20000: loss 2.136209\n",
      "iteration 600 / 20000: loss 2.078547\n",
      "iteration 700 / 20000: loss 2.119499\n",
      "iteration 800 / 20000: loss 2.068055\n",
      "iteration 900 / 20000: loss 2.105131\n",
      "iteration 1000 / 20000: loss 2.081134\n",
      "iteration 1100 / 20000: loss 2.043705\n",
      "iteration 1200 / 20000: loss 2.104410\n",
      "iteration 1300 / 20000: loss 2.107222\n",
      "iteration 1400 / 20000: loss 2.064030\n",
      "iteration 1500 / 20000: loss 2.049314\n",
      "iteration 1600 / 20000: loss 2.090758\n",
      "iteration 1700 / 20000: loss 2.123147\n",
      "iteration 1800 / 20000: loss 2.084675\n",
      "iteration 1900 / 20000: loss 2.057987\n",
      "iteration 2000 / 20000: loss 2.113973\n",
      "iteration 2100 / 20000: loss 2.185539\n",
      "iteration 2200 / 20000: loss 2.059389\n",
      "iteration 2300 / 20000: loss 2.105635\n",
      "iteration 2400 / 20000: loss 2.044850\n",
      "iteration 2500 / 20000: loss 2.098606\n",
      "iteration 2600 / 20000: loss 2.074498\n",
      "iteration 2700 / 20000: loss 2.070517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2800 / 20000: loss 2.084978\n",
      "iteration 2900 / 20000: loss 2.071425\n",
      "iteration 3000 / 20000: loss 2.144815\n",
      "iteration 3100 / 20000: loss 2.095687\n",
      "iteration 3200 / 20000: loss 2.150070\n",
      "iteration 3300 / 20000: loss 2.033679\n",
      "iteration 3400 / 20000: loss 2.063040\n",
      "iteration 3500 / 20000: loss 2.051307\n",
      "iteration 3600 / 20000: loss 2.141457\n",
      "iteration 3700 / 20000: loss 2.067237\n",
      "iteration 3800 / 20000: loss 2.117699\n",
      "iteration 3900 / 20000: loss 2.094276\n",
      "iteration 4000 / 20000: loss 2.077016\n",
      "iteration 4100 / 20000: loss 2.094106\n",
      "iteration 4200 / 20000: loss 2.108607\n",
      "iteration 4300 / 20000: loss 2.161802\n",
      "iteration 4400 / 20000: loss 2.045547\n",
      "iteration 4500 / 20000: loss 2.073278\n",
      "iteration 4600 / 20000: loss 2.040665\n",
      "iteration 4700 / 20000: loss 2.151971\n",
      "iteration 4800 / 20000: loss 2.106586\n",
      "iteration 4900 / 20000: loss 2.110584\n",
      "iteration 5000 / 20000: loss 2.070138\n",
      "iteration 5100 / 20000: loss 2.088530\n",
      "iteration 5200 / 20000: loss 2.055608\n",
      "iteration 5300 / 20000: loss 2.077251\n",
      "iteration 5400 / 20000: loss 2.021887\n",
      "iteration 5500 / 20000: loss 2.177606\n",
      "iteration 5600 / 20000: loss 2.061007\n",
      "iteration 5700 / 20000: loss 2.165748\n",
      "iteration 5800 / 20000: loss 2.093542\n",
      "iteration 5900 / 20000: loss 2.053389\n",
      "iteration 6000 / 20000: loss 2.086549\n",
      "iteration 6100 / 20000: loss 2.105660\n",
      "iteration 6200 / 20000: loss 2.127933\n",
      "iteration 6300 / 20000: loss 2.094140\n",
      "iteration 6400 / 20000: loss 2.128240\n",
      "iteration 6500 / 20000: loss 2.082703\n",
      "iteration 6600 / 20000: loss 2.087573\n",
      "iteration 6700 / 20000: loss 2.053330\n",
      "iteration 6800 / 20000: loss 2.100018\n",
      "iteration 6900 / 20000: loss 2.076564\n",
      "iteration 7000 / 20000: loss 2.076611\n",
      "iteration 7100 / 20000: loss 2.050293\n",
      "iteration 7200 / 20000: loss 2.068499\n",
      "iteration 7300 / 20000: loss 2.038055\n",
      "iteration 7400 / 20000: loss 2.073840\n",
      "iteration 7500 / 20000: loss 2.111137\n",
      "iteration 7600 / 20000: loss 2.125850\n",
      "iteration 7700 / 20000: loss 2.062322\n",
      "iteration 7800 / 20000: loss 2.111953\n",
      "iteration 7900 / 20000: loss 2.112442\n",
      "iteration 8000 / 20000: loss 2.106848\n",
      "iteration 8100 / 20000: loss 2.074992\n",
      "iteration 8200 / 20000: loss 2.026515\n",
      "iteration 8300 / 20000: loss 2.210578\n",
      "iteration 8400 / 20000: loss 2.081595\n",
      "iteration 8500 / 20000: loss 2.093497\n",
      "iteration 8600 / 20000: loss 2.103490\n",
      "iteration 8700 / 20000: loss 2.080878\n",
      "iteration 8800 / 20000: loss 1.996828\n",
      "iteration 8900 / 20000: loss 2.158230\n",
      "iteration 9000 / 20000: loss 2.096184\n",
      "iteration 9100 / 20000: loss 2.057319\n",
      "iteration 9200 / 20000: loss 2.064201\n",
      "iteration 9300 / 20000: loss 2.064046\n",
      "iteration 9400 / 20000: loss 2.053503\n",
      "iteration 9500 / 20000: loss 2.102965\n",
      "iteration 9600 / 20000: loss 2.122391\n",
      "iteration 9700 / 20000: loss 2.107433\n",
      "iteration 9800 / 20000: loss 2.070194\n",
      "iteration 9900 / 20000: loss 2.038784\n",
      "iteration 10000 / 20000: loss 2.102557\n",
      "iteration 10100 / 20000: loss 2.120122\n",
      "iteration 10200 / 20000: loss 2.161432\n",
      "iteration 10300 / 20000: loss 2.108044\n",
      "iteration 10400 / 20000: loss 2.063978\n",
      "iteration 10500 / 20000: loss 2.120999\n",
      "iteration 10600 / 20000: loss 2.057493\n",
      "iteration 10700 / 20000: loss 2.090836\n",
      "iteration 10800 / 20000: loss 2.100636\n",
      "iteration 10900 / 20000: loss 2.054238\n",
      "iteration 11000 / 20000: loss 2.183659\n",
      "iteration 11100 / 20000: loss 2.115564\n",
      "iteration 11200 / 20000: loss 2.121625\n",
      "iteration 11300 / 20000: loss 2.126254\n",
      "iteration 11400 / 20000: loss 2.084543\n",
      "iteration 11500 / 20000: loss 2.056577\n",
      "iteration 11600 / 20000: loss 2.045795\n",
      "iteration 11700 / 20000: loss 2.115330\n",
      "iteration 11800 / 20000: loss 2.081014\n",
      "iteration 11900 / 20000: loss 2.067596\n",
      "iteration 12000 / 20000: loss 2.058383\n",
      "iteration 12100 / 20000: loss 2.114995\n",
      "iteration 12200 / 20000: loss 2.153662\n",
      "iteration 12300 / 20000: loss 2.123698\n",
      "iteration 12400 / 20000: loss 2.112200\n",
      "iteration 12500 / 20000: loss 2.061577\n",
      "iteration 12600 / 20000: loss 2.051225\n",
      "iteration 12700 / 20000: loss 2.127540\n",
      "iteration 12800 / 20000: loss 2.017804\n",
      "iteration 12900 / 20000: loss 2.116893\n",
      "iteration 13000 / 20000: loss 2.063407\n",
      "iteration 13100 / 20000: loss 2.102905\n",
      "iteration 13200 / 20000: loss 2.082135\n",
      "iteration 13300 / 20000: loss 2.080845\n",
      "iteration 13400 / 20000: loss 2.069499\n",
      "iteration 13500 / 20000: loss 2.080625\n",
      "iteration 13600 / 20000: loss 2.095639\n",
      "iteration 13700 / 20000: loss 2.073360\n",
      "iteration 13800 / 20000: loss 2.068616\n",
      "iteration 13900 / 20000: loss 2.060298\n",
      "iteration 14000 / 20000: loss 2.154004\n",
      "iteration 14100 / 20000: loss 2.110345\n",
      "iteration 14200 / 20000: loss 2.124910\n",
      "iteration 14300 / 20000: loss 2.088647\n",
      "iteration 14400 / 20000: loss 2.143766\n",
      "iteration 14500 / 20000: loss 2.086310\n",
      "iteration 14600 / 20000: loss 2.179365\n",
      "iteration 14700 / 20000: loss 2.056519\n",
      "iteration 14800 / 20000: loss 2.057019\n",
      "iteration 14900 / 20000: loss 2.085978\n",
      "iteration 15000 / 20000: loss 2.146363\n",
      "iteration 15100 / 20000: loss 2.051336\n",
      "iteration 15200 / 20000: loss 2.075305\n",
      "iteration 15300 / 20000: loss 2.163450\n",
      "iteration 15400 / 20000: loss 2.106307\n",
      "iteration 15500 / 20000: loss 2.125346\n",
      "iteration 15600 / 20000: loss 2.084838\n",
      "iteration 15700 / 20000: loss 2.089678\n",
      "iteration 15800 / 20000: loss 2.093720\n",
      "iteration 15900 / 20000: loss 2.066398\n",
      "iteration 16000 / 20000: loss 2.047192\n",
      "iteration 16100 / 20000: loss 2.076219\n",
      "iteration 16200 / 20000: loss 2.123798\n",
      "iteration 16300 / 20000: loss 2.045494\n",
      "iteration 16400 / 20000: loss 2.093790\n",
      "iteration 16500 / 20000: loss 2.134734\n",
      "iteration 16600 / 20000: loss 2.036035\n",
      "iteration 16700 / 20000: loss 2.051548\n",
      "iteration 16800 / 20000: loss 2.134682\n",
      "iteration 16900 / 20000: loss 2.146065\n",
      "iteration 17000 / 20000: loss 2.110754\n",
      "iteration 17100 / 20000: loss 2.072988\n",
      "iteration 17200 / 20000: loss 2.115003\n",
      "iteration 17300 / 20000: loss 2.161839\n",
      "iteration 17400 / 20000: loss 2.132926\n",
      "iteration 17500 / 20000: loss 2.063823\n",
      "iteration 17600 / 20000: loss 2.114904\n",
      "iteration 17700 / 20000: loss 2.166032\n",
      "iteration 17800 / 20000: loss 2.137371\n",
      "iteration 17900 / 20000: loss 2.127716\n",
      "iteration 18000 / 20000: loss 2.092214\n",
      "iteration 18100 / 20000: loss 2.186358\n",
      "iteration 18200 / 20000: loss 2.077943\n",
      "iteration 18300 / 20000: loss 2.103054\n",
      "iteration 18400 / 20000: loss 2.100445\n",
      "iteration 18500 / 20000: loss 2.026817\n",
      "iteration 18600 / 20000: loss 2.045045\n",
      "iteration 18700 / 20000: loss 2.084893\n",
      "iteration 18800 / 20000: loss 2.139552\n",
      "iteration 18900 / 20000: loss 2.068117\n",
      "iteration 19000 / 20000: loss 2.190536\n",
      "iteration 19100 / 20000: loss 2.027862\n",
      "iteration 19200 / 20000: loss 2.108652\n",
      "iteration 19300 / 20000: loss 2.138740\n",
      "iteration 19400 / 20000: loss 2.103967\n",
      "iteration 19500 / 20000: loss 2.084217\n",
      "iteration 19600 / 20000: loss 2.086458\n",
      "iteration 19700 / 20000: loss 2.085121\n",
      "iteration 19800 / 20000: loss 2.127304\n",
      "iteration 19900 / 20000: loss 2.133278\n",
      "iteration 0 / 20000: loss 1552.152265\n",
      "iteration 100 / 20000: loss 2.217621\n",
      "iteration 200 / 20000: loss 2.129308\n",
      "iteration 300 / 20000: loss 2.104361\n",
      "iteration 400 / 20000: loss 2.164941\n",
      "iteration 500 / 20000: loss 2.100847\n",
      "iteration 600 / 20000: loss 2.137763\n",
      "iteration 700 / 20000: loss 2.145499\n",
      "iteration 800 / 20000: loss 2.185857\n",
      "iteration 900 / 20000: loss 2.150614\n",
      "iteration 1000 / 20000: loss 2.171845\n",
      "iteration 1100 / 20000: loss 2.106338\n",
      "iteration 1200 / 20000: loss 2.160406\n",
      "iteration 1300 / 20000: loss 2.153296\n",
      "iteration 1400 / 20000: loss 2.163122\n",
      "iteration 1500 / 20000: loss 2.182610\n",
      "iteration 1600 / 20000: loss 2.159462\n",
      "iteration 1700 / 20000: loss 2.148971\n",
      "iteration 1800 / 20000: loss 2.117486\n",
      "iteration 1900 / 20000: loss 2.151350\n",
      "iteration 2000 / 20000: loss 2.121683\n",
      "iteration 2100 / 20000: loss 2.146223\n",
      "iteration 2200 / 20000: loss 2.144353\n",
      "iteration 2300 / 20000: loss 2.109269\n",
      "iteration 2400 / 20000: loss 2.133494\n",
      "iteration 2500 / 20000: loss 2.129507\n",
      "iteration 2600 / 20000: loss 2.136190\n",
      "iteration 2700 / 20000: loss 2.158905\n",
      "iteration 2800 / 20000: loss 2.147192\n",
      "iteration 2900 / 20000: loss 2.186505\n",
      "iteration 3000 / 20000: loss 2.148055\n",
      "iteration 3100 / 20000: loss 2.208448\n",
      "iteration 3200 / 20000: loss 2.141443\n",
      "iteration 3300 / 20000: loss 2.182301\n",
      "iteration 3400 / 20000: loss 2.180970\n",
      "iteration 3500 / 20000: loss 2.104983\n",
      "iteration 3600 / 20000: loss 2.229963\n",
      "iteration 3700 / 20000: loss 2.113275\n",
      "iteration 3800 / 20000: loss 2.165504\n",
      "iteration 3900 / 20000: loss 2.170347\n",
      "iteration 4000 / 20000: loss 2.197291\n",
      "iteration 4100 / 20000: loss 2.152926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4200 / 20000: loss 2.137040\n",
      "iteration 4300 / 20000: loss 2.118609\n",
      "iteration 4400 / 20000: loss 2.145201\n",
      "iteration 4500 / 20000: loss 2.164522\n",
      "iteration 4600 / 20000: loss 2.152632\n",
      "iteration 4700 / 20000: loss 2.134313\n",
      "iteration 4800 / 20000: loss 2.196848\n",
      "iteration 4900 / 20000: loss 2.113574\n",
      "iteration 5000 / 20000: loss 2.184781\n",
      "iteration 5100 / 20000: loss 2.130999\n",
      "iteration 5200 / 20000: loss 2.128951\n",
      "iteration 5300 / 20000: loss 2.072045\n",
      "iteration 5400 / 20000: loss 2.142482\n",
      "iteration 5500 / 20000: loss 2.137923\n",
      "iteration 5600 / 20000: loss 2.130550\n",
      "iteration 5700 / 20000: loss 2.142185\n",
      "iteration 5800 / 20000: loss 2.121636\n",
      "iteration 5900 / 20000: loss 2.124258\n",
      "iteration 6000 / 20000: loss 2.158083\n",
      "iteration 6100 / 20000: loss 2.134494\n",
      "iteration 6200 / 20000: loss 2.155171\n",
      "iteration 6300 / 20000: loss 2.164416\n",
      "iteration 6400 / 20000: loss 2.146049\n",
      "iteration 6500 / 20000: loss 2.150300\n",
      "iteration 6600 / 20000: loss 2.148639\n",
      "iteration 6700 / 20000: loss 2.166821\n",
      "iteration 6800 / 20000: loss 2.151550\n",
      "iteration 6900 / 20000: loss 2.203942\n",
      "iteration 7000 / 20000: loss 2.132454\n",
      "iteration 7100 / 20000: loss 2.122277\n",
      "iteration 7200 / 20000: loss 2.152823\n",
      "iteration 7300 / 20000: loss 2.237633\n",
      "iteration 7400 / 20000: loss 2.134134\n",
      "iteration 7500 / 20000: loss 2.179507\n",
      "iteration 7600 / 20000: loss 2.147140\n",
      "iteration 7700 / 20000: loss 2.169762\n",
      "iteration 7800 / 20000: loss 2.150642\n",
      "iteration 7900 / 20000: loss 2.136916\n",
      "iteration 8000 / 20000: loss 2.194863\n",
      "iteration 8100 / 20000: loss 2.129920\n",
      "iteration 8200 / 20000: loss 2.115874\n",
      "iteration 8300 / 20000: loss 2.158953\n",
      "iteration 8400 / 20000: loss 2.167952\n",
      "iteration 8500 / 20000: loss 2.167443\n",
      "iteration 8600 / 20000: loss 2.133969\n",
      "iteration 8700 / 20000: loss 2.127433\n",
      "iteration 8800 / 20000: loss 2.166691\n",
      "iteration 8900 / 20000: loss 2.126910\n",
      "iteration 9000 / 20000: loss 2.131329\n",
      "iteration 9100 / 20000: loss 2.162570\n",
      "iteration 9200 / 20000: loss 2.133944\n",
      "iteration 9300 / 20000: loss 2.141688\n",
      "iteration 9400 / 20000: loss 2.187133\n",
      "iteration 9500 / 20000: loss 2.136384\n",
      "iteration 9600 / 20000: loss 2.118522\n",
      "iteration 9700 / 20000: loss 2.139100\n",
      "iteration 9800 / 20000: loss 2.184543\n",
      "iteration 9900 / 20000: loss 2.172319\n",
      "iteration 10000 / 20000: loss 2.138077\n",
      "iteration 10100 / 20000: loss 2.125424\n",
      "iteration 10200 / 20000: loss 2.196537\n",
      "iteration 10300 / 20000: loss 2.189868\n",
      "iteration 10400 / 20000: loss 2.152623\n",
      "iteration 10500 / 20000: loss 2.107960\n",
      "iteration 10600 / 20000: loss 2.154588\n",
      "iteration 10700 / 20000: loss 2.130803\n",
      "iteration 10800 / 20000: loss 2.130511\n",
      "iteration 10900 / 20000: loss 2.102711\n",
      "iteration 11000 / 20000: loss 2.164776\n",
      "iteration 11100 / 20000: loss 2.169564\n",
      "iteration 11200 / 20000: loss 2.153670\n",
      "iteration 11300 / 20000: loss 2.102816\n",
      "iteration 11400 / 20000: loss 2.172911\n",
      "iteration 11500 / 20000: loss 2.130412\n",
      "iteration 11600 / 20000: loss 2.156646\n",
      "iteration 11700 / 20000: loss 2.126096\n",
      "iteration 11800 / 20000: loss 2.133374\n",
      "iteration 11900 / 20000: loss 2.229533\n",
      "iteration 12000 / 20000: loss 2.134757\n",
      "iteration 12100 / 20000: loss 2.163910\n",
      "iteration 12200 / 20000: loss 2.129486\n",
      "iteration 12300 / 20000: loss 2.129093\n",
      "iteration 12400 / 20000: loss 2.185033\n",
      "iteration 12500 / 20000: loss 2.136665\n",
      "iteration 12600 / 20000: loss 2.131620\n",
      "iteration 12700 / 20000: loss 2.160348\n",
      "iteration 12800 / 20000: loss 2.111011\n",
      "iteration 12900 / 20000: loss 2.146401\n",
      "iteration 13000 / 20000: loss 2.176097\n",
      "iteration 13100 / 20000: loss 2.113663\n",
      "iteration 13200 / 20000: loss 2.229337\n",
      "iteration 13300 / 20000: loss 2.090800\n",
      "iteration 13400 / 20000: loss 2.076816\n",
      "iteration 13500 / 20000: loss 2.115502\n",
      "iteration 13600 / 20000: loss 2.169576\n",
      "iteration 13700 / 20000: loss 2.150337\n",
      "iteration 13800 / 20000: loss 2.154012\n",
      "iteration 13900 / 20000: loss 2.180882\n",
      "iteration 14000 / 20000: loss 2.143459\n",
      "iteration 14100 / 20000: loss 2.157878\n",
      "iteration 14200 / 20000: loss 2.141919\n",
      "iteration 14300 / 20000: loss 2.140974\n",
      "iteration 14400 / 20000: loss 2.150917\n",
      "iteration 14500 / 20000: loss 2.146530\n",
      "iteration 14600 / 20000: loss 2.150481\n",
      "iteration 14700 / 20000: loss 2.131374\n",
      "iteration 14800 / 20000: loss 2.136967\n",
      "iteration 14900 / 20000: loss 2.139287\n",
      "iteration 15000 / 20000: loss 2.124533\n",
      "iteration 15100 / 20000: loss 2.161541\n",
      "iteration 15200 / 20000: loss 2.144166\n",
      "iteration 15300 / 20000: loss 2.121998\n",
      "iteration 15400 / 20000: loss 2.171818\n",
      "iteration 15500 / 20000: loss 2.137401\n",
      "iteration 15600 / 20000: loss 2.175888\n",
      "iteration 15700 / 20000: loss 2.111098\n",
      "iteration 15800 / 20000: loss 2.154230\n",
      "iteration 15900 / 20000: loss 2.137984\n",
      "iteration 16000 / 20000: loss 2.163887\n",
      "iteration 16100 / 20000: loss 2.136346\n",
      "iteration 16200 / 20000: loss 2.131293\n",
      "iteration 16300 / 20000: loss 2.156776\n",
      "iteration 16400 / 20000: loss 2.115866\n",
      "iteration 16500 / 20000: loss 2.132395\n",
      "iteration 16600 / 20000: loss 2.153965\n",
      "iteration 16700 / 20000: loss 2.177594\n",
      "iteration 16800 / 20000: loss 2.114322\n",
      "iteration 16900 / 20000: loss 2.205017\n",
      "iteration 17000 / 20000: loss 2.122169\n",
      "iteration 17100 / 20000: loss 2.171533\n",
      "iteration 17200 / 20000: loss 2.104222\n",
      "iteration 17300 / 20000: loss 2.152056\n",
      "iteration 17400 / 20000: loss 2.201154\n",
      "iteration 17500 / 20000: loss 2.107811\n",
      "iteration 17600 / 20000: loss 2.155847\n",
      "iteration 17700 / 20000: loss 2.085809\n",
      "iteration 17800 / 20000: loss 2.167985\n",
      "iteration 17900 / 20000: loss 2.174178\n",
      "iteration 18000 / 20000: loss 2.184098\n",
      "iteration 18100 / 20000: loss 2.166307\n",
      "iteration 18200 / 20000: loss 2.141240\n",
      "iteration 18300 / 20000: loss 2.150655\n",
      "iteration 18400 / 20000: loss 2.112478\n",
      "iteration 18500 / 20000: loss 2.086211\n",
      "iteration 18600 / 20000: loss 2.090625\n",
      "iteration 18700 / 20000: loss 2.181232\n",
      "iteration 18800 / 20000: loss 2.084039\n",
      "iteration 18900 / 20000: loss 2.146660\n",
      "iteration 19000 / 20000: loss 2.180740\n",
      "iteration 19100 / 20000: loss 2.147450\n",
      "iteration 19200 / 20000: loss 2.150068\n",
      "iteration 19300 / 20000: loss 2.195620\n",
      "iteration 19400 / 20000: loss 2.125709\n",
      "iteration 19500 / 20000: loss 2.164469\n",
      "iteration 19600 / 20000: loss 2.159731\n",
      "iteration 19700 / 20000: loss 2.112599\n",
      "iteration 19800 / 20000: loss 2.169909\n",
      "iteration 19900 / 20000: loss 2.148105\n",
      "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.320388 val accuracy: 0.345000\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.310224 val accuracy: 0.326000\n",
      "lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.326449 val accuracy: 0.344000\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.291429 val accuracy: 0.309000\n",
      "best validation accuracy achieved during cross-validation: 0.345000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "num_class = 10\n",
    "for lr in learning_rates:\n",
    "    for reg in regularization_strengths:\n",
    "        softmax_classifier = Softmax()\n",
    "        softmax_classifier.train(X_train, y_train, lr, reg, 20000, verbose=True)\n",
    "        train_accuracy = np.mean(softmax_classifier.predict(X_train)==y_train)\n",
    "        val_accuracy = np.mean(softmax_classifier.predict(X_val)==y_val)\n",
    "        if(val_accuracy > best_val):\n",
    "            best_softmax = softmax_classifier\n",
    "            best_val = val_accuracy\n",
    "        results[(lr, reg)] = (train_accuracy, val_accuracy)\n",
    "        \n",
    "\n",
    "pass\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.333000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b88144bba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
